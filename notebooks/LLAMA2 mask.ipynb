{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffef1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    PromptEncoderConfig,\n",
    "    PeftModel,\n",
    "    PeftModelForSequenceClassification,\n",
    "    AutoPeftModelForSequenceClassification,\n",
    "    PeftConfig\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import bitsandbytes as bnb\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e67bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/mac9908/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d638fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for Unbalanced Dataset. Use default Trainer if not balanced.\n",
    "class InfraTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to('cuda')\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (we have 2 labels with greater weight on positive)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.5567, 4.9114], device=0))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "class LLMUtils:\n",
    "    def __init__(self):\n",
    "        self.metric = evaluate.load(\"f1\")\n",
    "        self.cf_metric = evaluate.load(\"BucketHeadP65/confusion_matrix\")\n",
    "        self.classification_report_metric = evaluate.load(\"bstrai/classification_report\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def ner_mask(self, x):\n",
    "        '''\n",
    "        '''\n",
    "        doc = self.nlp(x['comment'])\n",
    "        masked = x['comment']\n",
    "        for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting\n",
    "            if e.label_ in ['LOC', 'GPE']:\n",
    "                start = e.start_char\n",
    "                end = start + len(e.text)\n",
    "                masked = masked[:start] + '<LOCATION>' + masked[end:]\n",
    "        x['comment'] = masked\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ce4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMTrain():\n",
    "    def __init__(\n",
    "            self, \n",
    "            data,\n",
    "            config='/home/mac9908/InfrastructureOmbudsman/training/llm-config.json',\n",
    "            mask=True,\n",
    "            sample=None,\n",
    "            base_model=\"meta-llama/Llama-2-7b-hf\",\n",
    "            exp=\"LLAMA2_nomask\"\n",
    "        ):\n",
    "        self.metric = evaluate.load(\"f1\")\n",
    "        self.cf_metric = evaluate.load(\"BucketHeadP65/confusion_matrix\")\n",
    "        self.classification_report_metric = evaluate.load(\"bstrai/classification_report\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.device_map = {\"\": 0}\n",
    "        self.sample = sample\n",
    "        with open(config) as cfile:\n",
    "            config = json.loads(cfile.read())\n",
    "            for k, v in config.items():\n",
    "                setattr(self, k, v)\n",
    "        self.mask = mask\n",
    "        self.base_model = base_model\n",
    "        self.exp = exp\n",
    "        self.output_dir = os.path.join(os.getcwd(), f\"{base_model}_{exp}\")\n",
    "        self.datafile = data\n",
    "        \n",
    "    def tokenize_function(self, examples):\n",
    "        '''\n",
    "        '''\n",
    "        return self.tokenizer(examples[\"comment\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        '''\n",
    "        '''\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        f1 = self.metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "        return f1\n",
    "    def ner_mask(self, x):\n",
    "        '''\n",
    "        '''\n",
    "        doc = self.nlp(x['comment'])\n",
    "        masked = x['comment']\n",
    "        for e in reversed(doc.ents): #reversed to not modify the offsets of other entities when substituting\n",
    "            if e.label_ in ['LOC', 'GPE']:\n",
    "                start = e.start_char\n",
    "                end = start + len(e.text)\n",
    "                masked = masked[:start] + '<LOCATION>' + masked[end:]\n",
    "        x['comment'] = masked\n",
    "        return x\n",
    "    def load_dataset(self, tokenize=True):\n",
    "        '''\n",
    "        '''\n",
    "        # Load Dataset\n",
    "        df = pd.read_csv(self.datafile)\n",
    "        df['labels'] = df['label'].astype('int')\n",
    "        dt = datasets.Dataset.from_pandas(df)\n",
    "        if self.sample:\n",
    "            dt = dt.select(range(self.sample))\n",
    "        if self.mask:\n",
    "            dt = dt.map(self.ner_mask)\n",
    "        dt = dt.train_test_split(test_size=0.3, seed=42)\n",
    "        self.train_data = dt['train'].shuffle(seed=42)\n",
    "        self.test_data = dt['test'].shuffle(seed=42)\n",
    "        if tokenize:\n",
    "            print(f'Tokenizing dataset')\n",
    "            self.small_train = self.train_data.map(self.tokenize_function, batched=True, remove_columns=['id', 'comment', 'Unnamed: 0', 'label'])\n",
    "            self.small_test = self.test_data.map(self.tokenize_function, batched=True)\n",
    "    def load_qlora(self):\n",
    "        model_id = '/home/mac9908/InfrastructureOmbudsman/LLAMA2_mask'\n",
    "        self.model = AutoPeftModelForSequenceClassification.from_pretrained(model_id)\n",
    "        print(type(self.model))\n",
    "        self.model = self.model.merge_and_unload()\n",
    "        self.model.save_pretrained('LLAMA2_mask_merged')\n",
    "        print(type(self.model))\n",
    "    \n",
    "    def load_trainer(self):\n",
    "        # Load tokenizer and model with QLoRA configuration\n",
    "        compute_dtype = getattr(torch, self.bnb_4bit_compute_dtype)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=self.use_4bit,\n",
    "            bnb_4bit_quant_type=self.bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=self.use_nested_quant,\n",
    "        )\n",
    "\n",
    "        # Check GPU compatibility with bfloat16\n",
    "        if compute_dtype == torch.float16 and self.use_4bit:\n",
    "            major, _ = torch.cuda.get_device_capability()\n",
    "            if major >= 8:\n",
    "                print(\"=\" * 80)\n",
    "                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "                print(\"=\" * 80)\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.base_model,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=self.device_map,\n",
    "        )\n",
    "        # for name, module in model.named_modules():\n",
    "        #   print(name)\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.config.pretraining_tp = 1\n",
    "\n",
    "        \n",
    "        # # Load LLaMA tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model, trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        # tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "        self.load_dataset()\n",
    "        # Load LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=self.lora_alpha,\n",
    "            lora_dropout=self.lora_dropout,\n",
    "            r=self.lora_r,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "        )\n",
    "\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "\n",
    "        # Set training parameters\n",
    "        self.training_arguments = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=self.num_train_epochs,\n",
    "            per_device_train_batch_size=self.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
    "            optim=self.optim,\n",
    "            save_strategy='epoch',\n",
    "            logging_steps=self.logging_steps,\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "            evaluation_strategy='epoch',\n",
    "            fp16=self.fp16,\n",
    "            bf16=self.bf16,\n",
    "            max_grad_norm=self.max_grad_norm,\n",
    "            max_steps=self.max_steps,\n",
    "            warmup_ratio=self.warmup_ratio,\n",
    "            group_by_length=self.group_by_length,\n",
    "            lr_scheduler_type=self.lr_scheduler_type,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='eval_f1',\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        self.trainer = InfraTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.small_train,\n",
    "            eval_dataset=self.small_test,\n",
    "            tokenizer=self.tokenizer,\n",
    "            args=self.training_arguments,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "    def train(self):\n",
    "        self.load_trainer()\n",
    "        # Train model\n",
    "        self.trainer.train()\n",
    "        # Save trained model\n",
    "        self.trainer.save_model(f'{self.exp}')\n",
    "        self.tokenizer.save_pretrained(f'{self.exp}')\n",
    "    def predict(self, x):\n",
    "        x['predict'] = self.pipe(x['comment'])[0]['label']\n",
    "        return x\n",
    "    def do_eval(self, data):\n",
    "        pipe = pipeline(\n",
    "              task='text-classification',\n",
    "              model=self.model,\n",
    "              tokenizer=self.tokenizer,\n",
    "              padding=\"max_length\",\n",
    "              truncation=True,\n",
    "              max_length=512\n",
    "        )\n",
    "        def predict(x):\n",
    "            x['predict'] = pipe(x['comment'])[0]['label']\n",
    "            return x\n",
    "        res = data.map(predict)\n",
    "        res = res.to_pandas()\n",
    "        res['predict'] = res['predict'].map({'LABEL_0': 0, 'LABEL_1': 1})\n",
    "        print(f\"Precision Score: {sklearn.metrics.precision_score(res['label'], res['predict'])}\")\n",
    "        print(f\"Accuracy Score: {sklearn.metrics.accuracy_score(res['label'], res['predict'])}\")\n",
    "        print(f\"Balanced Accuracy Score: {sklearn.metrics.balanced_accuracy_score(res['label'], res['predict'])}\")\n",
    "        print(f\"F1: {sklearn.metrics.f1_score(res['label'], res['predict'])}\")\n",
    "        print(f\"Classification Report:\\n {sklearn.metrics.classification_report(res['label'], res['predict'])}\")\n",
    "        cm = sklearn.metrics.confusion_matrix(res['label'], res['predict'], labels=[0, 1])\n",
    "        disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "        return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67c9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LLMTrain(data='./data/infrastructure.csv', \n",
    "             mask=True,\n",
    "             exp='LLAMA2_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0d80be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720bd2c71b194f71a738b53279e5a5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d76d2e6f1d495a91413cb7ba9e341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79188bcc6f3e4e568b4a2462c81263f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1863 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94eafdaf1354ce19956886d4f038fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmac9908\u001b[0m (\u001b[33murban-data-science\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mac9908/InfrastructureOmbudsman/wandb/run-20240212_234733-yows0k09</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/urban-data-science/huggingface/runs/yows0k09' target=\"_blank\">floating-orchid-105</a></strong> to <a href='https://wandb.ai/urban-data-science/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/urban-data-science/huggingface' target=\"_blank\">https://wandb.ai/urban-data-science/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/urban-data-science/huggingface/runs/yows0k09' target=\"_blank\">https://wandb.ai/urban-data-science/huggingface/runs/yows0k09</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 3:53:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>0.558120</td>\n",
       "      <td>0.836969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.673334</td>\n",
       "      <td>0.913861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.651958</td>\n",
       "      <td>0.902387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.970244</td>\n",
       "      <td>0.923284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>1.152966</td>\n",
       "      <td>0.922058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64929766",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326ba9339f2e43dabd142ba10492371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.6666666666666666\n",
      "Accuracy Score: 0.9249061326658323\n",
      "Balanced Accuracy Score: 0.784092762321015\n",
      "F1: 0.6341463414634145\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96       713\n",
      "         1.0       0.67      0.60      0.63        86\n",
      "\n",
      "    accuracy                           0.92       799\n",
      "   macro avg       0.81      0.78      0.80       799\n",
      "weighted avg       0.92      0.92      0.92       799\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>labels</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>UgzURxNi_C77Tai4UJN4AaABAg</td>\n",
       "      <td>I remember living in Winooski VT and crossing ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>UgzPANj-taFusF56yS94AaABAg</td>\n",
       "      <td>Dear Mr. Buttigieg, My neighborhood is being o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218</td>\n",
       "      <td>UgxN7CIAOko_85UbEsB4AaABAg</td>\n",
       "      <td>The scariest Bridge I ever crossed was the &lt;LO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1872</td>\n",
       "      <td>UgzwkIaTzXxT0rb7Tjp4AaABAg</td>\n",
       "      <td>\"The eyes of history are on this appointment.\"...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751</td>\n",
       "      <td>Ugx_ul7oRYOC6LhtBQF4AaABAg</td>\n",
       "      <td>All of these government officials should head ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>353</td>\n",
       "      <td>UgzwxkzeSSsBbfyHXkt4AaABAg</td>\n",
       "      <td>I drive over that bridge several times going f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>337</td>\n",
       "      <td>UgwhH7BgHJjACLc0WBJ4AaABAg</td>\n",
       "      <td>Saw the \"design\" of the bridge. There was no s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>2098</td>\n",
       "      <td>jgulsr5</td>\n",
       "      <td>The rattling subway train from 1960s-70s that ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>668</td>\n",
       "      <td>UgxPAfdI6Nawn1k4m714AaABAg</td>\n",
       "      <td>Pete needs to be looking into this! They’ve do...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>2006</td>\n",
       "      <td>UgyU_c3fYlybGiAAKft4AaABAg</td>\n",
       "      <td>My husband and I were supposed to go to &lt;LOCAT...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                          id  \\\n",
       "0          2015  UgzURxNi_C77Tai4UJN4AaABAg   \n",
       "1          1600  UgzPANj-taFusF56yS94AaABAg   \n",
       "2           218  UgxN7CIAOko_85UbEsB4AaABAg   \n",
       "3          1872  UgzwkIaTzXxT0rb7Tjp4AaABAg   \n",
       "4           751  Ugx_ul7oRYOC6LhtBQF4AaABAg   \n",
       "..          ...                         ...   \n",
       "794         353  UgzwxkzeSSsBbfyHXkt4AaABAg   \n",
       "795         337  UgwhH7BgHJjACLc0WBJ4AaABAg   \n",
       "796        2098                     jgulsr5   \n",
       "797         668  UgxPAfdI6Nawn1k4m714AaABAg   \n",
       "798        2006  UgyU_c3fYlybGiAAKft4AaABAg   \n",
       "\n",
       "                                               comment  label  labels  predict  \n",
       "0    I remember living in Winooski VT and crossing ...    1.0       1        0  \n",
       "1    Dear Mr. Buttigieg, My neighborhood is being o...    0.0       0        0  \n",
       "2    The scariest Bridge I ever crossed was the <LO...    0.0       0        0  \n",
       "3    \"The eyes of history are on this appointment.\"...    0.0       0        0  \n",
       "4    All of these government officials should head ...    0.0       0        0  \n",
       "..                                                 ...    ...     ...      ...  \n",
       "794  I drive over that bridge several times going f...    0.0       0        1  \n",
       "795  Saw the \"design\" of the bridge. There was no s...    0.0       0        0  \n",
       "796  The rattling subway train from 1960s-70s that ...    1.0       1        0  \n",
       "797  Pete needs to be looking into this! They’ve do...    0.0       0        0  \n",
       "798  My husband and I were supposed to go to <LOCAT...    0.0       0        0  \n",
       "\n",
       "[799 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.do_eval(l.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb7f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac311fb58b642a693e79cf7effdecfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LABEL_0\n",
      "1      LABEL_0\n",
      "2      LABEL_0\n",
      "3      LABEL_0\n",
      "4      LABEL_0\n",
      "        ...   \n",
      "554    LABEL_1\n",
      "555    LABEL_0\n",
      "556    LABEL_0\n",
      "557    LABEL_0\n",
      "558    LABEL_0\n",
      "Name: predict, Length: 559, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085a9e72dbe64971aabf49fb79f72f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LABEL_0\n",
      "1      LABEL_0\n",
      "2      LABEL_0\n",
      "3      LABEL_0\n",
      "4      LABEL_0\n",
      "        ...   \n",
      "554    LABEL_0\n",
      "555    LABEL_1\n",
      "556    LABEL_0\n",
      "557    LABEL_0\n",
      "558    LABEL_0\n",
      "Name: predict, Length: 559, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f075e00bf74742b04cd8eb9cb10105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LABEL_0\n",
      "1      LABEL_0\n",
      "2      LABEL_0\n",
      "3      LABEL_0\n",
      "4      LABEL_1\n",
      "        ...   \n",
      "554    LABEL_0\n",
      "555    LABEL_1\n",
      "556    LABEL_0\n",
      "557    LABEL_0\n",
      "558    LABEL_1\n",
      "Name: predict, Length: 559, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5099743df514150b534b93da957771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LABEL_0\n",
      "1      LABEL_0\n",
      "2      LABEL_0\n",
      "3      LABEL_0\n",
      "4      LABEL_0\n",
      "        ...   \n",
      "554    LABEL_0\n",
      "555    LABEL_1\n",
      "556    LABEL_0\n",
      "557    LABEL_0\n",
      "558    LABEL_1\n",
      "Name: predict, Length: 559, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6e5c2f12564c43947e6b6c2f0f1573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LABEL_1\n",
      "1      LABEL_1\n",
      "2      LABEL_0\n",
      "3      LABEL_0\n",
      "4      LABEL_0\n",
      "        ...   \n",
      "554    LABEL_1\n",
      "555    LABEL_1\n",
      "556    LABEL_0\n",
      "557    LABEL_0\n",
      "558    LABEL_0\n",
      "Name: predict, Length: 559, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8120123672646467+-0.00015737554281164873</td>\n",
       "      <td>0.7876304545948807+-0.00015112942539167182</td>\n",
       "      <td>0.7990223724241613+-0.00013571373215346213</td>\n",
       "      <td>0.9259391771019677+-3.789030373046674e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    precision  \\\n",
       "0  0.8120123672646467+-0.00015737554281164873   \n",
       "\n",
       "                                       recall  \\\n",
       "0  0.7876304545948807+-0.00015112942539167182   \n",
       "\n",
       "                                           f1  \\\n",
       "0  0.7990223724241613+-0.00013571373215346213   \n",
       "\n",
       "                                    accuracy  \n",
       "0  0.9259391771019677+-3.789030373046674e-05  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(task='text-classification',\n",
    "              model=l.model,\n",
    "              tokenizer=l.tokenizer,\n",
    "              padding=\"max_length\",\n",
    "              truncation=True,\n",
    "              max_length=512)\n",
    "def eval_variance(data, pipe):\n",
    "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1, 'LABEL_0': 0, 'LABEL_1': 1}\n",
    "    def predict(x):\n",
    "        x['predict'] = pipe(x['comment'])[0]['label']\n",
    "        return x\n",
    "    res = data.map(predict)\n",
    "    res = res.to_pandas()\n",
    "    print(res['predict'])\n",
    "    res['predict'] = res['predict'].map(label2id)\n",
    "    rep = sklearn.metrics.classification_report(res['label'], res['predict'], output_dict=True)\n",
    "    precision, recall, f1, accuracy = rep['macro avg']['precision'], rep['macro avg']['recall'], rep['macro avg']['f1-score'], rep['accuracy']\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "mp, mr, mf, ma = [],[],[], []\n",
    "for i in range(5):\n",
    "  c = l.test_data.train_test_split(test_size=0.3, seed=i)\n",
    "  precision, recall, f1, accuracy = eval_variance(c['train'], pipe)\n",
    "  mp.append(precision)\n",
    "  mr.append(recall)\n",
    "  mf.append(f1)\n",
    "  ma.append(accuracy)\n",
    "llama2_mask_res = pd.DataFrame.from_dict({\n",
    "    'precision': [f'{np.mean(mp)}+-{np.var(mp)}'], \n",
    "    'recall': [f'{np.mean(mr)}+-{np.var(mr)}'], \n",
    "    'f1': [f'{np.mean(mf)}+-{np.var(mf)}'], \n",
    "    'accuracy': [f'{np.mean(ma)}+-{np.var(ma)}']})\n",
    "llama2_mask_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206e87ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79952fdc89c14e0fb58ae7192c47d3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d7986216604e219eb23da6916eb52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c7d38e7a4b4623bfba7636ca73300c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac9908/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LABEL_1\n",
      "1      LABEL_1\n",
      "2      LABEL_1\n",
      "3      LABEL_1\n",
      "4      LABEL_1\n",
      "        ...   \n",
      "794    LABEL_1\n",
      "795    LABEL_1\n",
      "796    LABEL_1\n",
      "797    LABEL_1\n",
      "798    LABEL_1\n",
      "Name: predict, Length: 799, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.553952321204517,\n",
       " 0.5014025245441796,\n",
       " 0.10019244628531151,\n",
       " 0.11013767209011265)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/infrastructure.csv')\n",
    "df['labels'] = df['label'].astype('int')\n",
    "dt = datasets.Dataset.from_pandas(df)\n",
    "dt = dt.map(l.ner_mask)\n",
    "dt = dt.train_test_split(test_size=0.3, seed=42)\n",
    "small_train = dt['train'].shuffle(seed=42)\n",
    "small_test = dt['test'].shuffle(seed=42)\n",
    "# small_train = small_train.map(tokenize_function, batched=True, remove_columns=['id', 'comment', 'Unnamed: 0', 'label'])\n",
    "# small_test = small_test.map(tokenize_function, batched=True)\n",
    "print(small_test['labels'][6])\n",
    "\n",
    "pipe = pipeline(task='text-classification',\n",
    "              model='/home/mac9908/InfrastructureOmbudsman/models/masked_infra_llama2_ft',\n",
    "              tokenizer=tokenizer,\n",
    "              padding=\"max_length\",\n",
    "              truncation=True,\n",
    "              max_length=512, device=0)\n",
    "def eval_variance(data, pipe):\n",
    "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1, 'LABEL_0': 0, 'LABEL_1': 1}\n",
    "    def predict(x):\n",
    "        x['predict'] = pipe(x['comment'])[0]['label']\n",
    "        return x\n",
    "    res = data.map(predict)\n",
    "    res = res.to_pandas()\n",
    "    print(res['predict'])\n",
    "    res['predict'] = res['predict'].map(label2id)\n",
    "    rep = sklearn.metrics.classification_report(res['label'], res['predict'], output_dict=True)\n",
    "    precision, recall, f1, accuracy = rep['macro avg']['precision'], rep['macro avg']['recall'], rep['macro avg']['f1-score'], rep['accuracy']\n",
    "    return precision, recall, f1, accuracy\n",
    "eval_variance(small_test, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b7409a1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pipe\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "del tokenizer\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082b3fbc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f902be609948bd883cebcccf4e2b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'llama2_nomask_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28meval\u001b[39m(\u001b[43mllama2_nomask_eval\u001b[49m\u001b[38;5;241m.\u001b[39msmall_test\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama2_nomask_eval' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pipe = pipeline(task='text-classification',\n",
    "      model='/home/mac9908/InfrastructureOmbudsman/models/masked_infra_llama2_ft',\n",
    "      tokenizer=tokenizer,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      max_length=512,\n",
    "      device=0)\n",
    "def predict(x):\n",
    "    x['predict'] = pipe(x['comment'])[0]['label']\n",
    "    return x\n",
    "def eval_variance(data):\n",
    "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1, 'LABEL_0': 0, 'LABEL_1': 1}\n",
    "    res = data.map(predict)\n",
    "    res = res.to_pandas()\n",
    "    print(pd.DataFrame.from_dict({'text': res['comment'], 'predict': res['predict'], 'label': res['label']}))\n",
    "    res['predict'] = res['predict'].map(label2id)\n",
    "    rep = sklearn.metrics.classification_report(res['label'], res['predict'], output_dict=True)\n",
    "    precision, recall, f1, accuracy = rep['macro avg']['precision'], rep['macro avg']['recall'], rep['macro avg']['f1-score'], rep['accuracy']\n",
    "    return precision, recall, f1, accuracy\n",
    "def eval(data):\n",
    "  res = data.map(predict)\n",
    "  res = res.to_pandas()\n",
    "  res['predict'] = res['predict'].map({'LABEL_0': 0, 'LABEL_1': 1})\n",
    "  print(f\"Precision Score: {sklearn.metrics.precision_score(res['label'], res['predict'])}\")\n",
    "  print(f\"Accuracy Score: {sklearn.metrics.accuracy_score(res['label'], res['predict'])}\")\n",
    "  print(f\"Balanced Accuracy Score: {sklearn.metrics.balanced_accuracy_score(res['label'], res['predict'])}\")\n",
    "  print(f\"F1: {sklearn.metrics.f1_score(res['label'], res['predict'])}\")\n",
    "  print(f\"Classification Report:\\n {sklearn.metrics.classification_report(res['label'], res['predict'])}\")\n",
    "  cm = sklearn.metrics.confusion_matrix(res['label'], res['predict'], labels=[0, 1])\n",
    "  disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "  disp.plot()\n",
    "  plt.show()\n",
    "  return res\n",
    "eval(llama2_nomask_eval.small_test.select(range(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4a4b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and clearing GPU memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Deleting and clearing GPU memory')\n",
    "del l\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fced5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
